Cubert v0.2.36
--------------
[NEW] Default build on Hadoop version 2.3.0
[NEW] Support for parameters supplied through executor properties (with "param." prefix)
[NEW] Encode and Decode operator: support for missing dictionary code
[NEW] Support for avro schema evolution. Missing values in union fields padded with default (=null) at read.
[NEW] MKDIR as a top level command in OnCompletion task
[NEW] Support for pig style single quotes (in addition to double quotes) for string arguments
[NEW] CreateBag aggregation function (UDF)
[NEW] InFile UDF (added in previous version) to test presence. Added support for checking presence in specific column.
[FEATURE] Resizable in-memory hash table used in Cube operator
[FEATURE] Resizability support for all memory efficient primitive array lists
[FEATURE] In-built typecast function can case input boolean values
[FEATURE] variable String arg support for UDF declaration
[FEATURE] REGISTER command supports loading jars from argument directory
[FEATURE] Cubert specific hadoop counters reported for Cube, Merge- and Hash-Join operators
[FEATURE] Runtime checks for duplicates found in dimension table un Reduce-Side Join operator
[FEATURE] Support for compression specific extensions for RUBIX files
[FEATURE] Compile time checks for duplicate (SORT/PARTITION) columns in shuffle statement
[FEATURE] Deep copy support for tuples with all data types
[BUG] Fixed case sensitivity in grammar for BLOCKGEN shuffle macros
[BUG] Fixed min value in MIN (double) aggregator
[BUG] Fixed aggregators to handle null values
[BUG] Fixed dropped column schemas in ENCODE and DECODE operators
[BUG] Merge Join operator: tuples with nested schema was not copied correctly

Cubert v0.2.21
--------------
[NEW] In memory columnar store now supports new column types: float, short, dictionary encoded strings and bags.
[NEW] Cubert now reports max/min/avg/median Mappers and Reducers time.
[NEW] Reduce-Side Join operator
[NEW] IsDistinct UDF added, that uses a HashSet to determine if the value is distinct.
[NEW] Can run multiple in a script from command line by providing multiple -x options
[NEW] In-memory columnar storage for tuples with primitive data types
[NEW] VIRTUAL input format that is not backed by filesystem
[FEATURE] Support ability to specify a user-defined shuffle rewriter
[FEATURE] Hadoop configuration properties can be specified for individual jobs
[FEATURE] PIVOT operator supports pivoting by number of rows
[FEATURE] TEE operator supports WITH SPLIT option to filter out the TEE'ed tuples
[FEATURE] CUBE operator supports boolean dimensions
[FEATURE] BLOCKGEN BY INDEX now uses round-robin assignment of blocks to reducers.
[FEATURE] VIRTUAL input storage now supports reporting progress.
[FEATURE] TEE writers (AVRO and TEXT) now supports compression.
[FEATURE] Map and Array types are handled correctly in AvroStorage.
[FEATURE] LOAD-CACHED operator supports "#LATEST" tag in path names
[FEATURE] On script compilation errors, Cubert throws an exception rather than exiting
[FEATURE] Cube operator has configurable hashtable size and flush threshold
[FEATURE] rubixtool can extract one block
[FEATURE] Caching Class.forName()
[BUG] Control characters as separator for TEXT writers are handled properly.
[BUG] CUBE operator: when flushing hash table, the values were not reset to initial values.
[BUG] The schemas from multi mappers are validated by the Semantic Analyzer.
[BUG] LazyTuple was not reporting field type correctly.
[BUG] BLOCKGEN BY INDEX now correctly handles keys whose hash code is Integer.MIN_VALUE
[BUG] RUBIX input format was not reporting progress correctly

Cubert v0.2.0
---------------
[NEW] Can use user-provided implementation of sort algorithm (the default is java.utils.Collections.sort)
[NEW] Added support for user-defined storage formats
[NEW] Added support for running Cubert script as Azkaban job
[NEW] User defined operators can request to cache files and Rubix file indices
[NEW] Added DICTINCT shuffle command
[NEW] Added DICTIONARY shuffle command
[NEW] Added RANK operator
[NEW] Added support for CUBE combo level and rollup
[CHANGE] Inline script dictionary is now created using keyword CREATE DICTIONARY
[CHANGE] ENCODE and DECODE operators now require the columns that are to be encoded/decoded
[CHANGE] Package org.apache.pig.piggybank (for AVRO storage) is renamed to com.linkedin.cubert.pig.piggybank to avoid name conflicts
[PERFORMANCE] Caching classname (string) -> Class mapping to improve runtime performance
[PERFORMANCE] New implementation of hash table (for HASH-JOIN operator) that is memory efficient
[BUG] Fixed bug where cached file analyzer was incorrectly treating the inline dictionary as path to a file
[BUG] TEXT output format was ignoring the field separator value
[BUG] User defined aggregation functions with default constructors were not instantiated correctly
[BUG] Fixed issues where distributed cached files were not handled properly in hadoop 2
